#set page(
  paper: "a4",
  margin: (left: 2.5cm, right: 2.5cm, top: 2cm, bottom: 2cm),
  numbering: "1",
)

#set text(
  font: "Times new roman",
  size: 12pt,
  lang: "ru",
)

#set heading(numbering: "1.")
#set enum(numbering: "1)")

// Импорт необходимых пакетов
#import "@preview/numbly:0.1.0": numbly

// ============================================================
// ТИТУЛЬНЫЙ ЛИСТ
// ============================================================

#set page(numbering: none)


#v(1cm)

#block(
  width: 100%,
  text(size: 14pt, weight: "bold", [
     #align(center)[Санкт-Петербургский Политехнический университет имени Петра Великого]
  ])
)

#v(1cm)

#block(
  width: 100%,
  text(size: 14pt, [
     #align(center)[Кафедра прикладной математики и информатики]
  ])
)

#v(3cm)

#block(
  width: 100%,
  text(size: 18pt, weight: "bold", [
     #align(center)[ОТЧЁТ]
  ])
)

#v(0.5cm)


#block(
  width: 100%,
  text(size: 16pt, weight: "bold", [
    #align(center)[
      *по лабораторной работе №1*
    ]
  ])
)

#v(1cm)

#align(center)[
    *ГИСТОГРАММЫ И ПЛОТНОСТИ РАСПРЕДЕЛЕНИЙ*
  ]

#v(2cm)

#block(
  width: 100%,
  inset: (left: 8cm),
  [
    Выполнил:\
    студент группы 5030102/30003 \
    Крутянский Роман Игоревич
  ]
)

#v(0.5cm)

#block(
  width: 100%,
  inset: (left: 8cm),
  [
    Преподаватель: Баженов Александр Николаевич
  ]
)

#v(5cm)

#align(center)[
  Санкт-Петербург\
  2025
]

#pagebreak()

#set page(numbering: "1")
#outline(title: [Содержание], indent: 1em)

#pagebreak()

= Введение

Целью первого блока лабораторных работ является знакомство с основными методами описательной статистики:
- визуализацией распределений,
- вычислением характеристик положения и рассеяния,
- анализом выбросов,
- построением эмпирических оценок функций распределения и плотности.

Исследуются следующие распределения:

+ Нормальное $N(x; 0, 1)$
+ Коши $C(x; 0, 1)$
+ Лапласа $L(x; 0, 1/sqrt(2))$
+ Пуассона $P(k; 5)$
+ Равномерное $U(x; -sqrt(3), sqrt(3))$


#pagebreak()

// ==================== ЛАБОРАТОРНАЯ РАБОТА №1 ====================
= Лабораторная работа №1: Гистограммы и плотности распределений

== Постановка задачи

*Цель:* Освоить принцип группировки данных и построения гистограмм, исследовать влияние размера выборки на определение характера распределения.

*Задача:* Сгенерировать выборки объёмом $n = 10, 100, 1000$ для каждого из 5 распределений и построить на одном графике гистограмму выборки и теоретическую кривую плотности распределения.

== Теоретическая часть

Гистограмма — это ступенчатая функция, аппроксимирующая плотность распределения вероятностей. Пусть имеется выборка $x_1, x_2, ..., x_n$. Интервал значений разбивается на $k$ бинов $[a_0, a_1), [a_1, a_2), ..., [a_(k-1), a_k]$.

Оценка плотности в $j$-м интервале:

$hat(f)(x) = n_j / (n h), quad x in [a_(j-1), a_j)]$

где $n_j$ — количество наблюдений в $j$-м интервале, $h = a_j - a_(j-1)$ — ширина интервала.

*Выбор количества бинов:*
- Слишком мало интервалов $->$ потеря информации
- Слишком много интервалов $->$ шум и переобучение

== Реализация

*Язык программирования:* Python 3.10 \
*Библиотеки:* NumPy, SciPy, Matplotlib

Основные методы:
- `scipy.stats` — генерация выборок из заданных распределений
- `matplotlib.pyplot.hist` с параметром `density=True` — построение нормированных гистограмм
- Аналитические функции плотности (`pdf`) — построение теоретических кривых

== Результаты

#figure(
  image("../results/lab1/normal_histograms.png", width: 100%),
  caption: [Нормальное распределение: гистограммы для n = 10, 100, 1000],
  // label: "fig:lab1-normal"
)

#figure(
  image("../results/lab1/cauchy_histograms.png", width: 100%),
  caption: [Распределение Коши: гистограммы для n = 10, 100, 1000],
  // label: "fig:lab1-cauchy"
)

#figure(
  image("../results/lab1/laplace_histograms.png", width: 100%),
  caption: [Распределение Лапласа: гистограммы для n = 10, 100, 1000],
  // label: "fig:lab1-laplace"
)

#figure(
  image("../results/lab1/poisson_histograms.png", width: 100%),
  caption: [Распределение Пуассона (λ = 5): гистограммы для n = 10, 100, 1000],
  // label: "fig:lab1-poisson"
)

#figure(
  image("../results/lab1/uniform_histograms.png", width: 100%),
  caption: [Равномерное распределение: гистограммы для n = 10, 100, 1000],
  // label: "fig:lab1-uniform"
)

== Обсуждение

*Нормальное распределение:* При $n = 10$ гистограмма плохо аппроксимирует теоретическую кривую из-за малого объёма выборки. При $n = 100$ форма становится узнаваемой, при $n = 1000$ наблюдается отличное совпадение.

*Распределение Коши:* Характеризуется тяжёлыми хвостами. При больших $n$ появляются экстремальные выбросы, что требует ограничения диапазона визуализации.

*Распределение Лапласа:* Более острый пик и тяжёлые хвосты по сравнению с нормальным распределением. Параметр масштаба $1/sqrt(2)$ обеспечивает дисперсию, равную 1.

*Распределение Пуассона ($lambda = 5$):* Дискретное распределение, поэтому используются бины с целочисленными границами. При $lambda = 5$ распределение асимметрично, но с ростом $n$ приближается к нормальному.

*Равномерное распределение:* Гистограмма демонстрирует плоскую форму, которая становится более выраженной с ростом $n$.

== Выводы
+ Размер выборки существенно влияет на качество аппроксимации: при $n >= 1000$ гистограммы хорошо согласуются с теоретическими распределениями.
+ Для распределений с тяжёлыми хвостами (Коши) требуется особая обработка выбросов.
+ Дискретные распределения требуют специального подхода к выбору бинов.

#pagebreak()

// ==================== ЛАБОРАТОРНАЯ РАБОТА №2 ====================
= Лабораторная работа №2: Характеристики положения и рассеяния

== Постановка задачи

*Цель:* Исследовать сходимость выборочных характеристик к теоретическим при росте $n$, оценить устойчивость различных оценок к выбросам.

*Задача:* Для выборок объёмом $n = 10, 100, 1000$ вычислить 5 характеристик положения:
+ Выборочное среднее $bar(x)$
+ Медиану $med$
+ Полусумму экстремальных элементов $z_R = (x_min + x_max) / 2$
+ Полусумму квартилей $z_Q = (z_(1/4) + z_(3/4)) / 2$
+ Усечённое среднее $z_tr$ (с отбрасыванием 10% наименьших и наибольших значений)

Повторить генерацию 1000 раз и найти:
$ E(z) = bar(z), quad D(z) = bar(z^2) - (bar(z))^2 $

*Формат вывода:* $x = E plus.minus sqrt(D)$ с округлением до первого знака после запятой.

== Теоретическая часть

*Выборочное среднее:*
$ bar(x) = 1/n sum_(i=1)^n x_i ) $

*Медиана:* центральное значение в упорядоченной выборке.

*Полусумма экстремумов (midrange):*
$z_R = (x_((1)) + x_((n))) / 2$

*Полусумма квартилей (midhinge):*
$z_Q = (Q_1 + Q_3) / 2$

*Усечённое среднее:*
$z_tr = 1/(n-2k) sum_(i=k+1)^(n-k) x_((i)), quad k = floor(0.1n)$

== Реализация

*Язык программирования:* Python 3.10 \
*Библиотеки:* NumPy, SciPy

Основные методы:
- `scipy.stats.rvs(size=n)` — генерация выборок
- `numpy.mean()`, `numpy.median()`, `numpy.percentile()` — вычисление характеристик
- `scipy.stats.trim_mean` — усечённое среднее
- Метод Монте-Карло (1000 повторений) для оценки $E(z)$ и $D(z)$

== Результаты

#figure(
  table(
    columns: (1fr, 1fr, 1fr, 1fr, 1fr, 1fr),
    stroke: (bottom: 1pt),
    align: center,

    [n], [Среднее], [Медиана], [Midrange], [Midhinge], [Trimmed],
    [10],
    [$-0.0 plus.minus 0.3$],
    [$-0.0 plus.minus 0.4$],
    [$-0.0 plus.minus 0.4$],
    [$-0.0 plus.minus 0.3$],
    [$-0.0 plus.minus 0.3$],

    [100],
    [$-0.0 plus.minus 0.1$],
    [$-0.0 plus.minus 0.1$],
    [$0.0 plus.minus 0.3$],
    [$-0.0 plus.minus 0.1$],
    [$-0.0 plus.minus 0.1$],

    [1000],
    [$-0.0 plus.minus 0.0$],
    [$-0.0 plus.minus 0.0$],
    [$-0.0 plus.minus 0.2$],
    [$-0.0 plus.minus 0.0$],
    [$-0.0 plus.minus 0.0$],
  ),
  caption: [Характеристики положения для нормального распределения],
)
#figure(
  table(
    columns: (1fr, 1fr, 1fr, 1fr, 1fr, 1fr),
    stroke: (bottom: 1pt),
    align: center,

    [n], [Среднее], [Медиана], [Midrange], [Midhinge], [Trimmed],
    [10],
    [$1.2 plus.minus 8.0$],
    [$-0.0 plus.minus 0.6$],
    [$6.1 plus.minus 17.9$],
    [$-0.0 plus.minus 0.9$],
    [$-0.1 plus.minus 1.4$],

    [100],
    [$0.3 plus.minus 5.5$],
    [$0.0 plus.minus 0.2$],
    [$14.0 plus.minus 38.6$],
    [$0.0 plus.minus 0.2$],
    [$0.0 plus.minus 0.2$],

    [1000],
    [$-2.5 plus.minus 10.5$],
    [$0.0 plus.minus 0.0$],
    [$-1291.9 plus.minus 234.2$],
    [$0.0 plus.minus 0.1$],
    [$0.0 plus.minus 0.1$],
  ),
  caption: [Характеристики положения для распределения Коши],
)

#figure(
  table(
    columns: (1fr, 1fr, 1fr, 1fr, 1fr, 1fr),
    stroke: (bottom: 1pt),
    align: center,

    [n], [Среднее], [Медиана], [Midrange], [Midhinge], [Trimmed],
    [10],
    [$0.0 plus.minus 0.3$],
    [$0.0 plus.minus 0.3$],
    [$0.0 plus.minus 0.6$],
    [$0.0 plus.minus 0.3$],
    [$0.0 plus.minus 0.3$],

    [100],
    [$0.0 plus.minus 0.1$],
    [$0.0 plus.minus 0.1$],
    [$-0.0 plus.minus 0.6$],
    [$0.0 plus.minus 0.1$],
    [$0.0 plus.minus 0.1$],

    [1000],
    [$-0.0 plus.minus 0.0$],
    [$-0.0 plus.minus 0.0$],
    [$-0.0 plus.minus 0.7$],
    [$-0.0 plus.minus 0.0$],
    [$-0.0 plus.minus 0.0$],
  ),
  caption: [Характеристики положения для распределения Лапласа],
)

#figure(
  table(
    columns: (1fr, 1fr, 1fr, 1fr, 1fr, 1fr),
    stroke: (bottom: 1pt),
    align: center,

    [n], [Среднее], [Медиана], [Midrange], [Midhinge], [Trimmed],
    [10],
    [$5.0 plus.minus 0.7$],
    [$4.8 plus.minus 0.9$],
    [$5.3 plus.minus 1.0$],
    [$4.9 plus.minus 0.8$],
    [$4.9 plus.minus 0.7$],

    [100],
    [$5.0 plus.minus 0.2$],
    [$4.9 plus.minus 0.3$],
    [$6.0 plus.minus 0.7$],
    [$4.9 plus.minus 0.4$],
    [$4.9 plus.minus 0.2$],

    [1000],
    [$5.0 plus.minus 0.1$],
    [$5.0 plus.minus 0.0$],
    [$6.8 plus.minus 0.6$],
    [$4.7 plus.minus 0.3$],
    [$4.9 plus.minus 0.1$],
  ),
  caption: [Характеристики положения для распределения Пуассона ($lambda = 5$)],
)

#figure(
  table(
    columns: (1fr, 1fr, 1fr, 1fr, 1fr, 1fr),
    stroke: (bottom: 1pt),
    align: center,

    [n], [Среднее], [Медиана], [Midrange], [Midhinge], [Trimmed],
    [10],
    [$-0.0 plus.minus 0.3$],
    [$-0.0 plus.minus 0.5$],
    [$-0.0 plus.minus 0.2$],
    [$-0.0 plus.minus 0.4$],
    [$-0.0 plus.minus 0.4$],

    [100],
    [$-0.0 plus.minus 0.1$],
    [$-0.0 plus.minus 0.2$],
    [$-0.0 plus.minus 0.0$],
    [$-0.0 plus.minus 0.1$],
    [$-0.0 plus.minus 0.1$],

    [1000],
    [$0.0 plus.minus 0.0$],
    [$0.0 plus.minus 0.1$],
    [$-0.0 plus.minus 0.0$],
    [$0.0 plus.minus 0.0$],
    [$0.0 plus.minus 0.0$],
  ),
  caption: [Характеристики положения для равномерного распределения],
)

#figure(
  image("../results/lab2/convergence_analysis.png", width: 100%),
  caption: [Зависимость дисперсии оценок от размера выборки],
)

== Обсуждение

*Нормальное распределение:* Все оценки сходятся к теоретическому центру $0$. Выборочное среднее демонстрирует наименьшую дисперсию (эффективность по теореме Гаусса–Маркова).

*Распределение Коши:*
- Среднее и midrange имеют огромную дисперсию, которая *не уменьшается* с ростом $n$ — следствие отсутствия теоретического математического ожидания
- Медиана, midhinge и trimmed mean устойчивы: их дисперсия закономерно убывает

*Распределение Лапласа:* Медиана демонстрирует эффективность, сопоставимую со средним, благодаря более тяжёлым хвостам.

*Распределение Пуассона ($lambda = 5$):* Midrange имеет систематическое смещение вправо из-за асимметрии распределения. Остальные оценки показывают хорошую сходимость.

*Равномерное распределение:* Midrange демонстрирует наименьшую дисперсию благодаря ограниченному носителю и симметрии.

== Выводы
+ Среднее арифметическое не всегда является лучшей оценкой центра распределения.
+ Для распределений с тяжёлыми хвостами (Коши) робастные оценки (медиана, усечённое среднее) существенно превосходят среднее.
+ Midrange эффективен только для распределений с ограниченным носителем.
+ С ростом объёма выборки все состоятельные оценки сходятся к истинному значению (кроме случаев, когда теоретические моменты не существуют).

#pagebreak()

// ==================== ЛАБОРАТОРНАЯ РАБОТА №3 ====================
= Лабораторная работа №3: Боксплот Тьюки и анализ выбросов

== Постановка задачи

*Цель:* Научиться применять боксплот Тьюки для анализа одномерных распределений, исследовать влияние размера выборки на долю отсеиваемых аномальных значений.

*Задача:*
+ Сгенерировать выборки объёмом $n = 20, 100$
+ Построить боксплот Тьюки
+ Экспериментально определить долю выбросов (1000 повторений)

== Теоретическая часть

*Межквартильный размах (IQR):*
$(I Q R = Q_3 - Q_1)$
*Границы выбросов (при $k = 1.5$):*
$
  "Lower" & = Q_1 - 1.5 dot.c I Q R \
  "Upper" & = Q_3 + 1.5 dot.c I Q R
$
*Доля выбросов:*
$ p_"out" = n_"out" / n $

== Реализация

*Язык программирования:* Python 3.10 \
*Библиотеки:* NumPy, SciPy, Matplotlib

Алгоритм:
+ Генерация 1000 выборок заданного объёма $n$
+ Вычисление $Q_1$, $Q_3$ и $ I Q R $ для каждой выборки
+ Определение границ выбросов: $[Q_1 - 1.5 dot.c I Q R; Q_3 + 1.5 dot.c I Q R]$
+ Подсчёт доли выбросов $p_"out" = n_"out" / n$
+ Усреднение по 1000 повторениям

== Результаты

#figure(
  image("../results/lab3/boxplots.png", width: 100%),
  caption: [Боксплоты Тьюки для всех распределений],
)

#figure(
  table(
    columns: (2fr, 1fr, 1fr, 1fr),
    stroke: (bottom: 1pt),
    align: center + horizon,

    [Распределение], [n], [Средняя доля], [Std],
    [Нормальное], [20], [$0.024$], [$0.045$],
    [], [100], [$0.010$], [$0.014$],
    [Коши], [20], [$0.155$], [$0.072$],
    [], [100], [$0.157$], [$0.035$],
    [Лаплас], [20], [$0.073$], [$0.066$],
    [], [100], [$0.066$], [$0.029$],
    [Пуассон ($lambda=5$)], [20], [$0.027$], [$0.049$],
    [], [100], [$0.015$], [$0.016$],
    [Равномерное], [20], [$0.002$], [$0.015$],
    [], [100], [$0.000$], [$0.000$],
  ),
  caption: [Средняя доля выбросов по методу Тьюки (1000 повторений)],
)

#figure(
  image("../results/lab3/outlier_proportion.png", width: 80%),
  caption: [Средняя доля выбросов в зависимости от размера выборки],
)

== Обсуждение

*Нормальное распределение:* Экспериментальная доля выбросов ($2.4%$ при $n=20$, $1.0%$ при $n=100$) близка к теоретическому значению $approx 0.7%$. Метод Тьюки работает корректно для распределений с лёгкими хвостами.

*Распределение Коши:* Доля выбросов стабильно высокая ($approx 15.6%$) — это особенность распределения с чрезвычайно тяжёлыми хвостами: точки, маркируемые как аномалии, являются типичными для закона Коши.

*Распределение Лапласа:* Доля выбросов ($6.6–7.3%$) занимает промежуточное положение, что согласуется с теорией: хвосты Лапласа тяжелее нормальных, но легче хвостов Коши.

*Распределение Пуассона ($lambda = 5$):* Доля выбросов ($1.5–2.7%$) превышает теоретическое значение $0.7%$ из-за правосторонней асимметрии: метод Тьюки маркирует как выбросы значения из длинного правого хвоста.

*Равномерное распределение:* Теоретическая доля выбросов равна нулю. Эксперимент подтверждает: $0.000%$ при $n=100$.

== Выводы
+ Метод Тьюки эффективен для обнаружения аномалий в распределениях с лёгкими хвостами.
+ Для распределений с тяжёлыми хвостами (Коши) метод даёт много «ложных» выбросов.
+ При интерпретации результатов необходимо учитывать природу распределения.
+ С ростом объёма выборки оценка доли выбросов становится более устойчивой.

#pagebreak()

// ==================== ЛАБОРАТОРНАЯ РАБОТА №4 ====================
= Лабораторная работа №4: Эмпирическая функция распределения и ядерные оценки плотности

== Постановка задачи

*Цель:* Исследовать сходимость эмпирической функции распределения к теоретической, сравнить ядерные оценки плотности с гистограммами.

*Задача:* Для выборок объёмом $n = 20, 60, 100$ построить:
+ Эмпирическую функцию распределения (ЭФР) и теоретическую функцию распределения
+ Ядерную оценку плотности (гауссово ядро) и теоретическую плотность

*Диапазоны:* $[-4, 4]$ для непрерывных распределений, $[6, 14]$ для Пуассона.

== Теоретическая часть

*Эмпирическая функция распределения:*
$ F_n(x) = 1/n sum_(i=1)^n I(x_i <= x) $
где $I(dot.c)$ — индикаторная функция.

*Теорема Гливенко–Кантелли:*
$ sup_x |F_n(x) - F(x)| arrow.r_(n arrow.r infinity) 0 $

*Ядерная оценка плотности (KDE):*
$ hat(f)_h(x) = 1/(n h) sum_(i=1)^n K((x - x_i)/h) ) $

Для гауссова ядра:
$ K(u) = 1/sqrt(2 pi) e^(-u^2 / 2) ) $

== Реализацияs

*Язык программирования:* Python 3.10 \
*Библиотеки:* NumPy, SciPy, Matplotlib, scikit-learn

Основные методы:
- `numpy.searchsorted` — эффективное вычисление ЭФР
- `sklearn.neighbors.KernelDensity` — ядерная оценка плотности с гауссовым ядром
- Автоматический выбор ширины окна по правилу Сильвермана: $h = 1.06 dot.c hat(sigma) dot.c n^(-1/5)$

== Результаты

#figure(
  image("../results/lab4/normal_edf_kde.png", width: 100%),
  caption: [Нормальное распределение: ЭФР и KDE],
)

#figure(
  image("../results/lab4/cauchy_edf_kde.png", width: 100%),
  caption: [Распределение Коши: ЭФР и KDE],
)

#figure(
  image("../results/lab4/laplace_edf_kde.png", width: 100%),
  caption: [Распределение Лапласа: ЭФР и KDE],
)

#figure(
  image("../results/lab4/poisson_edf_kde.png", width: 100%),
  caption: [Распределение Пуассона ($lambda = 5$): ЭФР и KDE],
)

#figure(
  image("../results/lab4/uniform_edf_kde.png", width: 100%),
  caption: [Равномерное распределение: ЭФР и KDE],
)

#figure(
  table(
    columns: (2fr, 1fr, 1fr, 1fr, 1fr),
    stroke: (bottom: 1pt),
    align: center,

    [Распределение], [n], [MSE (CDF)], [MSE (PDF)], [Std (PDF)],
    [Нормальное], [20], [$0.0036$], [$0.0025$], [$0.0020$],
    [], [60], [$0.0012$], [$0.0010$], [$0.0008$],
    [], [100], [$0.0007$], [$0.0007$], [$0.0005$],
    [Коши], [20], [$0.0073$], [$0.0026$], [$0.0017$],
    [], [60], [$0.0024$], [$0.0013$], [$0.0006$],
    [], [100], [$0.0014$], [$0.0011$], [$0.0004$],
    [Лаплас], [20], [$0.0034$], [$0.0046$], [$0.0027$],
    [], [60], [$0.0011$], [$0.0025$], [$0.0013$],
    [], [100], [$0.0006$], [$0.0019$], [$0.0009$],
    [Пуассон], [20], [$0.0043$], [---], [---],
    [], [60], [$0.0014$], [---], [---],
    [], [100], [$0.0008$], [---], [---],
    [Равномерное], [20], [$0.0033$], [$0.0045$], [$0.0019$],
    [], [60], [$0.0012$], [$0.0030$], [$0.0007$],
    [], [100], [$0.0007$], [$0.0025$], [$0.0005$],
  ),
  caption: [MSE для ЭФР и ядерной оценки плотности (500 повторений)],
)

== Обсуждение

*Эмпирическая функция распределения:*
- При $n = 20$ наблюдаются заметные отклонения, особенно в хвостах
- При $n = 100$ ЭФР практически совпадает с теоретической функцией
- Подтверждается теорема Гливенко–Кантелли

*Ядерная оценка плотности:*
- KDE даёт гладкую оценку в отличие от ступенчатой гистограммы
- При малых $n$ возможно избыточное сглаживание
- Для распределения Коши качество оценки ниже из-за тяжёлых хвостов

*Сравнение методов:*
- *Гистограмма:* проста, но зависит от выбора бинов, ступенчатая
- *KDE:* гладкая оценка, не требует выбора бинов, вычислительно сложнее
- При больших $n$ оба метода дают схожие результаты

== Выводы
+ Эмпирическая функция распределения сходится к теоретической с ростом объёма выборки.
+ Ядерные оценки обеспечивают гладкую аппроксимацию плотности, превосходящую гистограммы.
+ Для распределений с тяжёлыми хвостами требуется больший объём выборки для качественного приближения.
+ KDE предпочтительнее гистограмм для визуализации непрерывных распределений.

#pagebreak()

// ==================== ОБЩИЕ ВЫВОДЫ ====================
= Общие выводы по блоку лабораторных работ

В ходе выполнения блока лабораторных работ №1–4 были исследованы основные методы описательной статистики:

+ *Визуализация распределений:* Гистограммы позволяют оценить форму распределения, но качество аппроксимации сильно зависит от объёма выборки и количества бинов.
+ *Характеристики положения:* Выборочное среднее не всегда является оптимальной оценкой центра. Для распределений с тяжёлыми хвостами робастные оценки (медиана, усечённое среднее) существенно превосходят среднее по устойчивости.
+ *Анализ выбросов:* Метод Тьюки на основе IQR эффективен для распределений с лёгкими хвостами, но даёт много «ложных» выбросов для распределений с тяжёлыми хвостами (Коши).
+ *Эмпирические оценки:* ЭФР и KDE обеспечивают состоятельные оценки функций распределения и плотности. KDE предпочтительнее гистограмм благодаря гладкости и независимости от выбора бинов.

*Практическая значимость:* Полученные результаты демонстрируют важность выбора статистических методов в зависимости от природы данных. Для «нормальных» данных подходят классические методы, но при наличии тяжёлых хвостов или выбросов необходимо использовать робастные подходы.

#pagebreak()

= Список литературы
// Или вручную:
+ Гмурман В.Е. _Теория вероятностей и математическая статистика_. — 2003.
+ Кремер Н.Ш. _Теория вероятностей и математическая статистика_. — 2012.
+ Елисеева И.И., Юзбашев М.М. _Общая теория статистики_. — 2002.
+ Большев Л.Н., Смирнов Н.В. _Таблицы математической статистики_. — 1983.
+ Крамер Г. _Математические методы статистики_. — 1975.

#pagebreak()

// ==================== ПРИЛОЖЕНИЕ ====================
= Приложение. Ссылка на репозиторий GitHub

Исходный код программ, разработанных в ходе выполнения лабораторных работ, размещён в репозитории GitHub:

#align(center)[
  https://github.com/Fromant/matstat
]
